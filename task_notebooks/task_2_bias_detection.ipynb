{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 2: Bias Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Welcome to task 2!\n",
    "\n",
    "In this task you will build an LLM Judge to correctly classify the type of bias (if any) present in the content generated by an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Environment and Task Set Up \n",
    "\n",
    "Run the following cell. \n",
    "If there are no issues, you will get the message 'Root directory set up correctly!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -qq -r ../requirements.txt\n",
    "\n",
    "REL_PATH_TO_ROOT = \"../\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "sys.path.insert(0,REL_PATH_TO_ROOT)\n",
    "\n",
    "from src.utils import get_root_dir, test_root_dir\n",
    "from local_variables import ROOT_DIR\n",
    "\n",
    "test_root_dir(REL_PATH_TO_ROOT)\n",
    "\n",
    "from prompt_manager.manager import PromptManager\n",
    "from prompt_manager.fetcher import fetch_prompt\n",
    "from src.api import generate_outputs_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "The dataset contains 30 extracts of company advertisements generated by an LLM.\n",
    "\n",
    "Several of the extracts contain a stereotype bias. These can be classified as one of the following types:\n",
    "- Race\n",
    "- Profession\n",
    "- Gender\n",
    "- Age\n",
    "\n",
    "The column 'extract' contains the advertisement extract, and 'target' contains the classified stereotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(REL_PATH_TO_ROOT, 'data', 'bias_dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Task: Build LLM as a Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Craft a prompt that aims to correctly categorise the type of stereotype.\n",
    "\n",
    "The **input** to your LLM Judge is the extract.\n",
    "\n",
    "The **output** from your LLM Judge should be a classification: 'gender', 'race', 'profession', 'age' or 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompt\n",
    "SEQUENCE = [\"task_2\",\"bias_classifier\"]\n",
    "prompt_template = fetch_prompt(SEQUENCE,use_latest_version=True)\n",
    "print(f\"Current LLM Judge Prompt:\\n------------------------\\n{prompt_template}\\n------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply prompt to dataset\n",
    "evaluator_responses = []\n",
    "\n",
    "for _, row in tqdm.tqdm(df.iterrows()):\n",
    "\n",
    "    # Get inputs and place into dictionary format\n",
    "    context = row[\"extract\"]\n",
    "    row_inputs = {\"CONTEXT\" : context}\n",
    "\n",
    "    # Initialise prompt to validate and format inputs\n",
    "    prompt = PromptManager(template=prompt_template,inputs=row_inputs)\n",
    "    prompt.validate_inputs()\n",
    "    prompt.format_inputs()\n",
    "\n",
    "    # Send prompt and collect response\n",
    "    response = generate_outputs_openai(prompt.prompt)\n",
    "    evaluator_responses.append(response)\n",
    "\n",
    "df[\"evaluator_bias_classification\"] = evaluator_responses\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get agreement\n",
    "agreement_counts = [1 if str(row['target']) == str(row['evaluator_bias_classification']) else 0 for _, row in df.iterrows()]\n",
    "percentage_agreement = sum(agreement_counts)/len(agreement_counts)\n",
    "print(f\"\\n Your LLM Judge achieved {round(100 * percentage_agreement, 1)}% agreement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### End of Exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
