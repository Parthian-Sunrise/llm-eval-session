{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 2: Bias Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Welcome to task 2!\n",
    "\n",
    "Whether generated by human or AI, it is important that companies avoid introducing bias into any marketing or promotional material. All communications to customers should maintain neutrality and avoid perpetuation of social biases. However, this is often not the case, and a powerful use case for an LLM Judge is to identify types of bias present in such information before they reach customers.\n",
    "\n",
    "In this task you will build an LLM Judge to correctly classify the type of bias (if any) present in company advertisements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#### Definition of Bias\n",
    "\n",
    "*Social bias broadly encompasses disparate treatment or outcomes between social groups. This could entail representational harms such as stereotyping, misrepresentation, toxic language and exclusionary norms.*\n",
    "\n",
    "In this exercise we will focus on **Stereotyping** and in particular towards the following social groups:\n",
    "\n",
    "##### **Gender Stereotypes**\n",
    "This involves stereotypes or assumptions based on a person’s gender. For example, assuming that men are more suited for leadership roles or that women are more caring. Gender bias often reflects traditional, societal roles that place limitations on individuals based solely on their gender\n",
    "\n",
    "##### **Racial Stereotypes**\n",
    "This type of bias involves prejudices or stereotypes related to a person’s race or ethnicity. Examples include assuming certain racial groups are more athletic, technologically adept, or prone to certain behaviors. Racial bias can contribute to harmful generalizations that limit opportunities and reinforce stereotypes.\n",
    "\n",
    "##### **Age Stereotypes**\n",
    "Age bias involves assumptions or stereotypes based on someone’s age. This can include ideas that older individuals are less tech-savvy or that younger people lack experience and maturity. Age bias often affects hiring, career advancement, and general perceptions of competence or suitability based solely on age.\n",
    "\n",
    "##### **Profession Stereotypes**\n",
    "Profession bias includes assumptions about people based on their job or career. For instance, assuming that all teachers are women or that mathematicians are geeks. This bias can lead to stereotypical views of what kinds of people belong in certain professions, often limiting career possibilities based on assumptions rather than skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Environment and Task Set Up \n",
    "\n",
    "Run the following cell. \n",
    "If there are no issues, you will get the message 'Root directory set up correctly!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -qq -r ../requirements.txt\n",
    "\n",
    "REL_PATH_TO_ROOT = \"../\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "sys.path.insert(0,REL_PATH_TO_ROOT)\n",
    "\n",
    "from src.utils import get_root_dir, test_root_dir\n",
    "from local_variables import ROOT_DIR\n",
    "\n",
    "test_root_dir(REL_PATH_TO_ROOT)\n",
    "\n",
    "from prompt_manager.manager import PromptManager\n",
    "from prompt_manager.fetcher import fetch_prompt\n",
    "from src.api import generate_outputs_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "The dataset contains 30 extracts of company advertisements (these are all fictional)\n",
    "\n",
    "Several of the extracts contain one of the above stereotype biases, classified as one of the following types:\n",
    "- Race\n",
    "- Profession\n",
    "- Gender\n",
    "- Age\n",
    "\n",
    "The column *'extract'* contains the advertisement extract, and *'target'* contains the bias classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(REL_PATH_TO_ROOT, 'data', 'bias_dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Task: Build LLM as a Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Craft a prompt that aims to correctly categorise the type of stereotype.\n",
    "\n",
    "The **input** to your LLM Judge is the extract.\n",
    "\n",
    "The **output** from your LLM Judge should be a classification: 'gender', 'race', 'profession', 'age' or 'none' if the information is unbiased.\n",
    "\n",
    "Use the code to calculate accuracy of your judge against the ground truth classifications! How high can you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Load the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompt\n",
    "SEQUENCE = [\"task_2\",\"bias_classifier\"]\n",
    "prompt_template = fetch_prompt(SEQUENCE,use_latest_version=True)\n",
    "print(f\"Current LLM Judge Prompt:\\n------------------------\\n{prompt_template}\\n------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Apply the prompt to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_responses = []\n",
    "\n",
    "for _, row in tqdm.tqdm(df.iterrows()):\n",
    "\n",
    "    # Get inputs and place into dictionary format\n",
    "    context = row[\"extract\"]\n",
    "    row_inputs = {\"CONTEXT\" : context}\n",
    "\n",
    "    # Initialise prompt to validate and format inputs\n",
    "    prompt = PromptManager(template=prompt_template,inputs=row_inputs)\n",
    "    prompt.validate_inputs()\n",
    "    prompt.format_inputs()\n",
    "\n",
    "    # Send prompt and collect response\n",
    "    response = generate_outputs_openai(prompt.prompt)\n",
    "    evaluator_responses.append(response)\n",
    "\n",
    "df[\"evaluator_bias_classification\"] = evaluator_responses\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Get Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get agreement\n",
    "agreement_counts = [1 if str(row['target']) == str(row['evaluator_bias_classification']) else 0 for _, row in df.iterrows()]\n",
    "percentage_agreement = sum(agreement_counts)/len(agreement_counts)\n",
    "print(f\"\\n Your LLM Judge achieved {round(100 * percentage_agreement, 1)}% agreement!\\n\")\n",
    "print(\" Try tweaking your prompt and rescoring the test set to reach further alignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### End of Exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
