{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 1: Hallucination Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Welcome to task 1\n",
    "\n",
    "In this task you will build an LLM Judge to analyse whether the provided answer is a hallucination with given context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Environment Set Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory set up correctly!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -qq -r ../requirements.txt\n",
    "\n",
    "REL_PATH_TO_ROOT = \"../\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0,REL_PATH_TO_ROOT)\n",
    "\n",
    "from src.utils import get_root_dir, test_root_dir\n",
    "from local_variables import ROOT_DIR\n",
    "\n",
    "test_root_dir(REL_PATH_TO_ROOT)\n",
    "\n",
    "from prompt_manager.manager import PromptManager\n",
    "from prompt_manager.fetcher import fetch_prompt\n",
    "from src.api import generate_outputs_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Load Hallucination Dataset\n",
    "\n",
    "The dataset contains 50 question-answer pairs.\n",
    "\n",
    "For each question-answer pair, we have provided ground truth labels for hallucination. A '1' suggests this answer is a correct answer, and a '0' suggests this is a hallucinated answer. There are 25 pairs with label 1 and 25 with label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(REL_PATH_TO_ROOT, \"data/hallucination_final.csv\")\n",
    "hallucination_df = pd.read_csv(input_path).drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "      <th>Ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He transferred to Wolverhampton Wanderers for...</td>\n",
       "      <td>1948 and 1964</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No. 11 Squadron is a Royal Australian Air Forc...</td>\n",
       "      <td>RAAF Base Edinburgh</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kim Roi-ha (born November 15, 1965) is a South...</td>\n",
       "      <td>Memories of Murder</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The film stars Jeremy Blackman, Tom Cruise, M...</td>\n",
       "      <td>Jason Robards</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Greatest Hits: Back to the Start is the second...</td>\n",
       "      <td>Megadeth</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context             Response  \\\n",
       "0   He transferred to Wolverhampton Wanderers for...        1948 and 1964   \n",
       "1  No. 11 Squadron is a Royal Australian Air Forc...  RAAF Base Edinburgh   \n",
       "2  Kim Roi-ha (born November 15, 1965) is a South...   Memories of Murder   \n",
       "3   The film stars Jeremy Blackman, Tom Cruise, M...        Jason Robards   \n",
       "4  Greatest Hits: Back to the Start is the second...             Megadeth   \n",
       "\n",
       "   Ground_truth  \n",
       "0          True  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4          True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Task: Build LLM-as-a-judge\n",
    "\n",
    "For each metric, craft a prompt that aims to correctly categorise whether the response is a correct answer or a hallucinated one.\n",
    "\n",
    "The inputs to your LLM Judge should be the context and the response.\n",
    "\n",
    "The output from your LLM Judge should be a boolean TRUE/FALSE categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LLM Judge Prompt:\n",
      "------------------------\n",
      "Here's the context:\n",
      "{CONTEXT}\n",
      "\n",
      "Here's what the AI said:\n",
      "\n",
      "{RESPONSE}\n",
      "\n",
      "Was the response a hallucination?\n",
      "\n",
      "Say 'True' if the response is correct and 'False' if it's a hallucination and nothing else!\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get prompt\n",
    "SEQUENCE = [\"task_1\",\"hallucination_detector\"]\n",
    "\n",
    "prompt_template = fetch_prompt(SEQUENCE,use_latest_version=True)\n",
    "\n",
    "print(f\"Current LLM Judge Prompt:\\n------------------------\\n{prompt_template}\\n------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150bb722-fd4f-4b14-a311-ed028d3d226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of rows to process\n",
    "num_rows = 3  # Set to the desired number of rows or None to run all\n",
    "\n",
    "# Determine the total number of rows in the DataFrame\n",
    "total_rows = len(hallucination_df)\n",
    "\n",
    "# Check if num_rows is None, indicating that we want to process all rows\n",
    "if num_rows is None:\n",
    "    rows_to_process = total_rows\n",
    "else:\n",
    "    # Otherwise, set rows_to_process to the smaller of num_rows and total_rows\n",
    "    rows_to_process = min(num_rows, total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b1dcb6-bd2d-47cf-a385-1e96965ac45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  1.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "      <th>Ground_truth</th>\n",
       "      <th>evaluator_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He transferred to Wolverhampton Wanderers for...</td>\n",
       "      <td>1948 and 1964</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No. 11 Squadron is a Royal Australian Air Forc...</td>\n",
       "      <td>RAAF Base Edinburgh</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kim Roi-ha (born November 15, 1965) is a South...</td>\n",
       "      <td>Memories of Murder</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context             Response  \\\n",
       "0   He transferred to Wolverhampton Wanderers for...        1948 and 1964   \n",
       "1  No. 11 Squadron is a Royal Australian Air Forc...  RAAF Base Edinburgh   \n",
       "2  Kim Roi-ha (born November 15, 1965) is a South...   Memories of Murder   \n",
       "\n",
       "   Ground_truth evaluator_response  \n",
       "0          True               True  \n",
       "1          True               True  \n",
       "2          True               True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep track of model responses\n",
    "evaluator_responses = []\n",
    "\n",
    "# Loop through dataset with a row limit if specified\n",
    "for i, (_, row) in enumerate(tqdm(hallucination_df.head(rows_to_process).iterrows())):\n",
    "    \n",
    "    # Get inputs and place into dictionary format\n",
    "    context = row[\"Context\"]\n",
    "    response = row[\"Response\"]\n",
    "    row_inputs = {\"CONTEXT\": context, \"RESPONSE\": response}\n",
    "\n",
    "    # Initialise prompt to validate and format inputs\n",
    "    prompt = PromptManager(template=prompt_template, inputs=row_inputs)\n",
    "    prompt.validate_inputs()\n",
    "    prompt.format_inputs()\n",
    "\n",
    "    # Send prompt and collect response\n",
    "    response = generate_outputs_openai(prompt.prompt)\n",
    "    evaluator_responses.append(response)\n",
    "\n",
    "# Create a new DataFrame with only processed rows and add the evaluator responses\n",
    "processed_df = hallucination_df.head(rows_to_process).copy()\n",
    "processed_df[\"evaluator_response\"] = evaluator_responses\n",
    "\n",
    "# Display the resulting processed DataFrame\n",
    "display(processed_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now, calculates the accuracy of your LLM-as-a-judge. How does it look? Would you consider doing more prompt engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ded0ce-fd3e-4dc8-92e7-0436758c573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your LLM Judge achieved 100.0% agreement!\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store whether each prediction matches\n",
    "agreement_counts = []\n",
    "\n",
    "# Loop through each row in the DataFrame to compare values\n",
    "for _, row in processed_df.iterrows():\n",
    "    # Convert both values to strings for comparison to handle potential data type mismatches\n",
    "    hallucination_value = str(row['Ground_truth'])\n",
    "    evaluator_response_value = str(row['evaluator_response'])\n",
    "    \n",
    "    # Check if the values match exactly\n",
    "    if hallucination_value == evaluator_response_value:\n",
    "        agreement_counts.append(1)  # Add 1 to indicate agreement\n",
    "    else:\n",
    "        agreement_counts.append(0)  # Add 0 to indicate disagreement\n",
    "\n",
    "# Calculate the percentage of agreements\n",
    "total_comparisons = len(agreement_counts)\n",
    "number_of_agreements = sum(agreement_counts)\n",
    "percentage_agreement = number_of_agreements / total_comparisons\n",
    "\n",
    "# Display the result as a percentage\n",
    "percentage_agreement_rounded = round(100 * percentage_agreement, 1)\n",
    "print(f\"\\nYour LLM Judge achieved {percentage_agreement_rounded}% agreement!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get accuracy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m agreement_counts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhallucination\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluator_response\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m      3\u001b[0m percentage_agreement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(agreement_counts)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(agreement_counts)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Your LLM Judge achieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mpercentage_agreement,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% agreement!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Get accuracy\n",
    "agreement_counts = [1 if str(row['hallucination']) == str(row['evaluator_response']) else 0 for _, row in df.iterrows()]\n",
    "percentage_agreement = sum(agreement_counts)/len(agreement_counts)\n",
    "print(f\"\\n Your LLM Judge achieved {round(100 * percentage_agreement, 1)}% agreement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## End of Task 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-eval-venv)",
   "language": "python",
   "name": "llm-eval-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
