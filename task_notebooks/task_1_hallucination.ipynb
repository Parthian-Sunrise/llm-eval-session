{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 1: Hallucination Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Welcome to task 1\n",
    "\n",
    "In this task you will build an LLM Judge to analyse whether the provided answer is a hallucination with given context and relevant information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Environment Set Up \n",
    "\n",
    "Run the following cell. \n",
    "If there are no issues, you will get the message 'Root directory set up correctly!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -qq -r ../requirements.txt\n",
    "\n",
    "REL_PATH_TO_ROOT = \"../\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0,REL_PATH_TO_ROOT)\n",
    "\n",
    "from src.utils import get_root_dir, test_root_dir\n",
    "from local_variables import ROOT_DIR\n",
    "\n",
    "test_root_dir(REL_PATH_TO_ROOT)\n",
    "\n",
    "from prompt_manager.manager import PromptManager\n",
    "from prompt_manager.fetcher import fetch_prompt\n",
    "from src.api import generate_outputs_openai\n",
    "from src.image_display import display_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Task Background\n",
    "\n",
    "Below is the initial ask from the AskAI team as well as an explanation of the dataset they have provided you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### The Ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{get_root_dir()}/task_images/task_1_desc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{get_root_dir()}/task_images/task_1_data.png\",max_size=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "The dataset contains 50 question-answer pairs.\n",
    "\n",
    "For each question-answer pair, we have provided ground truth labels for hallucination. \n",
    "\n",
    "A label of 1 suggests this answer is incorrect and is a hallucination.\n",
    "A label of 0 suggests the answer is correct.\n",
    "\n",
    "There are 25 question-answer pairs with label 1 and 25 pairs with label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(REL_PATH_TO_ROOT, \"data/hallucination.csv\")\n",
    "hallucination_df = pd.read_csv(input_path).drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Task: Build LLM-as-a-judge\n",
    "\n",
    "Craft a prompt that aims to correctly categorise whether the response is a correct answer or a hallucinated one.\n",
    "\n",
    "The **inputs** to your LLM Judge is the context (i.e. relevant_knowledge and user_question) and the response.\n",
    "\n",
    "The **output** from your LLM Judge should be a boolean 1/0 categorisation\n",
    "\n",
    "An initial prompt has already been created for you to start from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Load the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE = [\"task_1\",\"hallucination_detector\"]\n",
    "\n",
    "prompt_template = fetch_prompt(SEQUENCE,use_latest_version=True)\n",
    "\n",
    "print(f\"Current LLM Judge Prompt:\\n------------------------\\n{prompt_template}\\n------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of rows to process\n",
    "num_rows = 50  # Set to the desired number of rows or None to run all\n",
    "\n",
    "# Define context based on our data for the initial prompt\n",
    "hallucination_df[\"context\"] = hallucination_df[\"relevant_knowledge\"] + hallucination_df[\"user_question\"]\n",
    "\n",
    "# Define response based on our data for the initial prompt\n",
    "hallucination_df[\"response\"] = hallucination_df[\"chatbot_answer\"]\n",
    "\n",
    "# Determine the total number of rows in the DataFrame\n",
    "total_rows = len(hallucination_df)\n",
    "\n",
    "# Check if num_rows is None, indicating that we want to process all rows\n",
    "if num_rows is None:\n",
    "    rows_to_process = total_rows\n",
    "else:\n",
    "    # Otherwise, set rows_to_process to the smaller of num_rows and total_rows\n",
    "    rows_to_process = min(num_rows, total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of model responses\n",
    "evaluator_responses = []\n",
    "\n",
    "# Loop through dataset with a row limit if specified\n",
    "for i, (_, row) in enumerate(tqdm(hallucination_df.head(rows_to_process).iterrows())):\n",
    "    \n",
    "    # Get inputs and place into dictionary format\n",
    "    context = row[\"context\"]\n",
    "    response = row[\"response\"]\n",
    "    row_inputs = {\"CONTEXT\": context, \"RESPONSE\": response}\n",
    "\n",
    "    # Initialise prompt to validate and format inputs\n",
    "    prompt = PromptManager(template=prompt_template, inputs=row_inputs)\n",
    "    prompt.validate_inputs()\n",
    "    prompt.format_inputs()\n",
    "\n",
    "    # Send prompt and collect response\n",
    "    response = generate_outputs_openai(prompt.prompt)\n",
    "    evaluator_responses.append(response)\n",
    "\n",
    "# Create a new DataFrame with only processed rows and add the evaluator responses\n",
    "processed_df = hallucination_df.head(rows_to_process).copy()\n",
    "processed_df[\"evaluator_response\"] = evaluator_responses\n",
    "\n",
    "# Display the resulting processed DataFrame\n",
    "display(processed_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now, calculate the accuracy of your LLM-as-a-judge. How does it look? Should you consider doing more prompt engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[\"agreement\"] = (processed_df[\"is_hallucination_error_ground_truth\"].astype(str) == processed_df[\"evaluator_response\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_agreement = processed_df[\"agreement\"].mean()\n",
    "percentage_agreement_rounded = round(100 * percentage_agreement, 1)\n",
    "print(f\"\\nYour LLM Judge achieved {percentage_agreement_rounded}% agreement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## End of Task 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
