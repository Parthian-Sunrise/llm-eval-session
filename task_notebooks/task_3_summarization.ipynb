{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 3: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Welcome to task 3!\n",
    "\n",
    "LLMs can be effective at assessing the quality of summarised information when compared to the original text. This involves leveraging their ability to understand, compare and evaluate textual content.\n",
    "\n",
    "This approach can be used to assess AI-generated summaries for refinement, compare human-written summaries for quality assurance and provide feedback to improve summarisation tools.\n",
    "\n",
    "By using structured prompts, examples, and specific criteria, LLMs can serve as powerful tools for evaluating the quality of summaries in a consistent and scalable way.\n",
    "\n",
    "**In this task you will build an LLM Judge to analyse the quality of summaries provided by an LLM summarizer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Environment and Task Set Up \n",
    "\n",
    "Run the following cell. \n",
    "If there are no issues, you will get the message 'Root directory set up correctly!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -qq -r ../requirements.txt\n",
    "\n",
    "REL_PATH_TO_ROOT = \"../\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "sys.path.insert(0,REL_PATH_TO_ROOT)\n",
    "\n",
    "from src.utils import get_root_dir, test_root_dir\n",
    "from local_variables import ROOT_DIR\n",
    "\n",
    "test_root_dir(REL_PATH_TO_ROOT)\n",
    "\n",
    "from prompt_manager.manager import PromptManager\n",
    "from prompt_manager.fetcher import fetch_prompt\n",
    "from src.api import generate_outputs_openai\n",
    "from src.image_display import display_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Task Background\n",
    "\n",
    "Below is the initial ask from the Journalist-Mini team as well as an explanation of the dataset they have provided you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### The Ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{get_root_dir()}/task_images/task_3_desc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(f\"{get_root_dir()}/task_images/task_3_data.png\",max_size=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "The dataset contains 30 CNN news articles along with summaries of the articles.\n",
    "\n",
    "For each summary, we have provided ground truth labels for 3 measures of summary quality:\n",
    "\n",
    "- Fluency\n",
    "- Brevity\n",
    "- Coverage\n",
    "\n",
    "Definitions for these can be found in task_notebooks/summarization_metrics.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./summarization_metrics.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(REL_PATH_TO_ROOT, \"data/summarisation.csv\")\n",
    "df = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Task: Build LLM as a Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "For each metric, craft a prompt that aims to capture the evaluation rubric\n",
    "\n",
    "The **inputs** to your LLM Judge should be the article and/or the article summary.\n",
    "\n",
    "The **output** from your LLM Judge is a scoring system of your choice, however you should produce a final score.\n",
    "\n",
    "The **ground truths** are binary and should serve as a guide for benchmarking but unlike previous exercises, you cannot calculate direct agreement unless you choose your metric scale to be binary as well (this is where your own human review could help you!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Fluency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##### Load the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE = [\"task_3\",\"fluency\"]\n",
    "prompt_template = fetch_prompt(SEQUENCE,use_latest_version=True)\n",
    "print(f\"Current LLM Judge Prompt:\\n------------------------\\n{prompt_template}\\n------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "##### Apply the prompt to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_responses = []\n",
    "\n",
    "for _, row in tqdm.tqdm(df.iterrows()):\n",
    "\n",
    "    # Get inputs and place into dictionary format\n",
    "    summary = row[\"journalist_mini_summary\"]\n",
    "    row_inputs = {\"SUMMARY\" : summary}\n",
    "\n",
    "    # Initialise prompt to validate and format inputs\n",
    "    prompt = PromptManager(template=prompt_template,inputs=row_inputs)\n",
    "    prompt.validate_inputs()\n",
    "    prompt.format_inputs()\n",
    "\n",
    "    # Send prompt and collect response\n",
    "    response = generate_outputs_openai(prompt.prompt)\n",
    "    evaluator_responses.append(response)\n",
    "\n",
    "df[\"evaluator_fluency\"] = evaluator_responses\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"journalist_mini_summary\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "##### Load the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE = [\"task_3\",\"brevity\"]\n",
    "prompt_template = fetch_prompt(SEQUENCE,use_latest_version=True)\n",
    "print(f\"Current LLM Judge Prompt:\\n------------------------\\n{prompt_template}\\n------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### Apply the prompt to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_responses = []\n",
    "\n",
    "for _, row in tqdm.tqdm(df.iterrows()):\n",
    "\n",
    "    # Get inputs and place into dictionary format\n",
    "    summary = row[\"journalist_mini_summary\"]\n",
    "    row_inputs = {\"SUMMARY\" : summary}\n",
    "\n",
    "    # Initialise prompt to validate and format inputs\n",
    "    prompt = PromptManager(template=prompt_template,inputs=row_inputs)\n",
    "    prompt.validate_inputs()\n",
    "    prompt.format_inputs()\n",
    "\n",
    "    # Send prompt and collect response\n",
    "    response = generate_outputs_openai(prompt.prompt)\n",
    "    evaluator_responses.append(response)\n",
    "\n",
    "df[\"evaluator_brevity\"] = evaluator_responses\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "##### Load the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE = [\"task_3\",\"coverage\"]\n",
    "prompt_template = fetch_prompt(SEQUENCE,use_latest_version=True)\n",
    "print(f\"Current LLM Judge Prompt:\\n------------------------\\n{prompt_template}\\n------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "##### Apply the prompt to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_responses = []\n",
    "\n",
    "for _, row in tqdm.tqdm(df.iterrows()):\n",
    "\n",
    "    # Get inputs and place into dictionary format\n",
    "    summary = row[\"journalist_mini_summary\"]\n",
    "    article = row[\"original_article\"]\n",
    "    row_inputs = {\"SUMMARY\" : summary, \"ARTICLE\" : article}\n",
    "\n",
    "    # Initialise prompt to validate and format inputs\n",
    "    prompt = PromptManager(template=prompt_template,inputs=row_inputs)\n",
    "    prompt.validate_inputs()\n",
    "    prompt.format_inputs()\n",
    "\n",
    "    # Send prompt and collect response\n",
    "    response = generate_outputs_openai(prompt.prompt)\n",
    "    evaluator_responses.append(response)\n",
    "\n",
    "df[\"evaluator_coverage\"] = evaluator_responses\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "If you have time, consider adding other metrics to your metric suite for summary quality.\n",
    "\n",
    "For example, you could consider metrics such as:\n",
    "\n",
    "- Hallucination (can you re-use your previous prompt?)\n",
    "- Formatting including dates, special characters etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### End of Task 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
