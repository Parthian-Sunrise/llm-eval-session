{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 0 - Environment Set Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 1 - Project Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.1 - Map to Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_PATH_TO_ROOT = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,REL_PATH_TO_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_root_dir, test_root_dir\n",
    "from local_variables import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_dir(REL_PATH_TO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 1.2 - Set-Up Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_manager.manager import PromptManager\n",
    "from prompt_manager.fetcher import fetch_prompt\n",
    "from src.api import generate_outputs_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 1.3 - Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in dataset of articles\n",
    "summar_df = pd.read_csv(f\"{get_root_dir()}/data/summarization.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "summar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# 2 - Generating Ground Truths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 2.1 - Choice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify metric names for summarisation\n",
    "METRICS = [\"fluency\",\"brevity\",\"coverage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise ground truth columns for these metrics in the dataframe\n",
    "summar_df[METRICS] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random generator that decides whether or not there should be an error of a given metric type\n",
    "def ground_truth_generation(metric_list):\n",
    "    metric_ground_truths = {metric : bool(rand.randint(0,1)) for metric in metric_list}\n",
    "    return metric_ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example output, false = no error, true = error present\n",
    "ground_truth_generation(METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the base prompt we use to generate the summary from the article\n",
    "\n",
    "# ARTICLE is the placeholder for the article to be summarised\n",
    "\n",
    "# Each rule is an instruction to the AI to either obey the rule or to break it, see prompts in prompts/dev/summarisation_prompts\n",
    "core_prompt = \"\"\"\n",
    "\n",
    "Your job is a summariser, working for a special organisation. Your job is to write a SUMMARY for the ARTICLE, however you must follow SPECIAL RULES set by the organisation. \n",
    "\n",
    "Your summary is constrained by 3 Rules: FLUENCY, BREVITY, COVERAGE. When writing the summary obey the rules as stipulated in the RULES section:\n",
    "\n",
    "**ARTICLE**:\n",
    "{ARTICLE}\n",
    "\n",
    "**BREVITY RULE**\n",
    "Brevity: {brev_rule}\n",
    "\n",
    "**COVERAGE RULE**\n",
    "Coverage: {cov_rule}\n",
    "\n",
    "**FLUENCY RULE**:\n",
    "Fluency: {flu_rule}\n",
    "\n",
    "WRITE YOUR SUMMARY FOLLOWING THE RULES ABOVE\n",
    "\n",
    "Return your summary:\n",
    "\n",
    "Chop chop!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a map from the metrics themselves to their rule placeholders\n",
    "metric_to_core_prompt_placeholder ={\"fluency\" : \"{flu_rule}\", \"brevity\" : \"{brev_rule}\", \"coverage\" : \"{cov_rule}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import rules to make errors or avoid them (error or unerror respectively)\n",
    "from prompts.dev.summarisation_prompts import error_prompts, unerror_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create choice engine\n",
    "\n",
    "def choice_engine(core_prompt, metric_list,ground_truth, article, error_prompts, unerror_prompts, metric_to_core_prompt_placeholder, article_placeholder=\"{ARTICLE}\"):\n",
    "    \"\"\"\n",
    "    Generate system prompt for OpenAI response with summary of an article with known errors (or lack thereof) based on desired ground truth\n",
    "\n",
    "    Args:\n",
    "        core_prompt (str): system prompt used to generate OpenAi response.\n",
    "        metric_list (list): list of metrics for summarisation\n",
    "        ground_truth (dict): dictionary of metric list values as keys and the values being whether or not an error should be introduced for that metric (True - error, False - no error)\n",
    "        article (str): text of the article to be summarised\n",
    "        error_prompts (dict): list of prompts corresponding to known errors for the summarisation metrics (same keys as metric_list values)\n",
    "        unerror_prompts (dict): list of prompts corresponding to avoiding errors for the summarisation metrics (same keys as metric_list values)\n",
    "        metric_to_core_prompt_placeholder (dict): specify a map between the values in metric_list and the placeholder for the (un)error prompts to be substituted into the core prompt\n",
    "        article_placeholder (str): name of the placeholder in the core prompt where the article should be substituted \n",
    "\n",
    "    Returns:\n",
    "        core_prompt: returns core_prompt with appropriate formatting absed on ground truth\n",
    "        \"\"\"\n",
    "    \n",
    "    core_prompt = core_prompt.replace(article_placeholder,article)\n",
    "\n",
    "    for key, error_bool in zip(ground_truth.keys(), ground_truth.values()):\n",
    "        placeholder = metric_to_core_prompt_placeholder[key]\n",
    "        \n",
    "        if error_bool:\n",
    "            metric_rule = error_prompts[key]\n",
    "        else:\n",
    "            metric_rule = unerror_prompts[key]\n",
    "\n",
    "        core_prompt = core_prompt.replace(placeholder,metric_rule)\n",
    "\n",
    "    return core_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_responses = []\n",
    "\n",
    "summar_df = summar_df.sample(n=30,random_state=42)\n",
    "\n",
    "# Loop through dataset\n",
    "for index, row in summar_df.iterrows():\n",
    "\n",
    "    # Get row level inputs\n",
    "    article = row[\"article\"]\n",
    "\n",
    "    # Get random ground truth for this row\n",
    "    ground_truth = ground_truth_generation(METRICS)\n",
    "\n",
    "    # Loop through the ground truth metrics and assign the ground truth of an error to the row in the dataframe\n",
    "    for key in ground_truth.keys():\n",
    "        summar_df.at[index, key] = ground_truth[key]\n",
    "   \n",
    "\n",
    "    # Format prompt for ground truth\n",
    "    prompt = choice_engine(core_prompt,METRICS,ground_truth,article,error_prompts,unerror_prompts,metric_to_core_prompt_placeholder)\n",
    "    \n",
    "    # Send prompt and collect response\n",
    "    response = generate_outputs_openai(prompt)\n",
    "    evaluator_responses.append(response)\n",
    "\n",
    "\n",
    "summar_df[\"altered_summaries\"] = evaluator_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_copy = summar_df.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure grammar errors but relooping\n",
    "\n",
    "fluency_error_prompt = \"\"\"Introduce spelling, grammar and punctuation errors in the following TEXT:\n",
    "{TEXT_PLACEHOLDER}\n",
    "\n",
    "Return the errored text below:\n",
    "\"\"\"\n",
    "\n",
    "# Loop through dataset\n",
    "for index, row in summar_df.iterrows():\n",
    "\n",
    "    # Get row level inputs\n",
    "    summary = row[\"altered_summaries\"]\n",
    "\n",
    "    # Get random ground truth for this row\n",
    "    ground_truth = row[\"fluency\"]\n",
    "\n",
    "    # Loop through the ground truth metrics and assign the ground truth of an error to the row in the dataframe\n",
    "    if ground_truth:\n",
    "        prompt = fluency_error_prompt.replace(\"TEXT_PLACEHOLDER\",summary)\n",
    "        # Send prompt and collect response\n",
    "        response = generate_outputs_openai(prompt)\n",
    "        summar_df.at[index, 'altered_summaries'] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = summar_df[[\"article\",\"fluency\",\"brevity\",\"coverage\",\"altered_summaries\"]].copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = processed_df.rename(columns={\"article\" : \"original_article\", \"altered_summaries\" : \"journalist_mini_summary\", \"fluency\" : \"is_fluency_error_ground_truth\", \"brevity\" : \"is_brevity_error_ground_truth\", \"coverage\" : \"is_coverage_error_ground_truth\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv(f\"{get_root_dir()}/data/summarisation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
